import type {
  AIProvider,
  AIStreamChunk,
  AIProviderGenerateParams,
  AIProviderTransformParams,
} from '../provider-interface.js';

/**
 * EchoProvider â€” mock/dev AI provider for testing
 *
 * Generates content by echoing the prompt with some formatting.
 * Simulates streaming by yielding tokens with a small delay.
 */
export class EchoProvider implements AIProvider {
  readonly id = 'echo';
  readonly displayName = 'Echo (Development)';

  async *generate(params: AIProviderGenerateParams): AsyncIterable<AIStreamChunk> {
    const response = this.buildGenerateResponse(params.prompt, params.context, params.mode, params.selectedText, params.cursorContext);
    yield* this.streamResponse(response);
  }

  async *transform(params: AIProviderTransformParams): AsyncIterable<AIStreamChunk> {
    const response = this.buildTransformResponse(
      params.selectedText,
      params.instruction
    );
    yield* this.streamResponse(response);
  }

  async validateConfig(): Promise<boolean> {
    return true;
  }

  private buildGenerateResponse(prompt: string, context?: string, mode?: string, selectedText?: string, cursorContext?: string): string {
    if (mode === 'analyse') {
      const lines = [
        `**Analysis Feedback**\n`,
        `Based on your request: "${prompt.slice(0, 200)}${prompt.length > 200 ? '...' : ''}"`,
        '',
      ];
      if (context) {
        lines.push(`Document reviewed (${context.length} chars).\n`);
      }
      lines.push(
        'This is a development analysis from the Echo provider.',
        'In production, the AI would provide constructive feedback about the document.',
        '',
        '- The document structure looks reasonable',
        '- Consider adding more detail to the introduction',
        '- Tone is consistent throughout',
      );
      return lines.join('\n');
    }

    if (mode === 'replace' && selectedText) {
      // When selection is present, return only the replacement for that section
      const lines = [
        `${selectedText}`,
        '',
        `<!-- Rewritten per: ${prompt.slice(0, 100)} -->`,
      ];
      return lines.join('\n');
    }

    if (mode === 'replace' && context) {
      const lines = [
        context,
        '',
        `<!-- Applied: ${prompt.slice(0, 100)} -->`,
      ];
      return lines.join('\n');
    }

    const lines = [
      `**AI Generated Content**\n`,
      `Based on your request: "${prompt.slice(0, 200)}${prompt.length > 200 ? '...' : ''}"`,
      '',
    ];

    if (context) {
      lines.push(`*Using document context (${context.length} chars)*\n`);
    }

    if (selectedText) {
      lines.push(`*User selected text (${selectedText.length} chars): "${selectedText.slice(0, 100)}${selectedText.length > 100 ? '...' : ''}"*\n`);
    } else if (cursorContext) {
      lines.push(`*Cursor near: "${cursorContext.slice(0, 100)}${cursorContext.length > 100 ? '...' : ''}"*\n`);
    }

    lines.push(
      'This is a development response from the Echo provider.',
      'In production, this would be generated by a real AI model.',
      '',
      `> ${prompt}`
    );

    return lines.join('\n');
  }

  private buildTransformResponse(selectedText: string, instruction: string): string {
    const lines = [
      `**Transformed Content** (${instruction})\n`,
      `Original (${selectedText.length} chars): "${selectedText.slice(0, 100)}${selectedText.length > 100 ? '...' : ''}"`,
      '',
      `*Applied: ${instruction}*\n`,
      selectedText,
    ];

    return lines.join('\n');
  }

  private async *streamResponse(text: string): AsyncIterable<AIStreamChunk> {
    // Simulate token-by-token streaming with word-level chunks
    const words = text.split(/(\s+)/);
    let totalTokens = 0;

    for (const word of words) {
      if (word.length > 0) {
        totalTokens++;
        yield { type: 'token', content: word };
        // Small delay to simulate streaming (10ms per token)
        await new Promise((resolve) => setTimeout(resolve, 10));
      }
    }

    yield {
      type: 'done',
      content: text,
      metadata: {
        tokensUsed: totalTokens,
        model: 'echo-v1',
        finishReason: 'stop',
      },
    };
  }
}
